{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d6b1c46",
   "metadata": {},
   "source": [
    "### Mike Ogrysko\n",
    "### CS 766 Information Retrieval and Natural Language Processing\n",
    "\n",
    "Processing the IMDB movie reviews for sentiment analysis\n",
    "- IMDB movie review data\n",
    "- Extract keywords that help to differentiate between reviews labeled as sentiment 0 and reviews labeled as sentiment 1 - Tf-Idf features to classify reviews using an SVM classifier\n",
    "- Rank the first 10 keywords that indicate the difference between the classes 0 and 1\n",
    "- Cluster the reviews into two groups and classify and report the 10-fold CV classification performance\n",
    "- Compare keywords generated by Tf-Idf features and keywords generated by clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a39f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, adjusted_rand_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b44ea4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews loaded 50000\n",
      "Total sentiments loaded 50000\n"
     ]
    }
   ],
   "source": [
    "Reviews, Sentiments = [], []\n",
    "\n",
    "with open('movie_data.csv','r', encoding='utf8') as fin:\n",
    "    reader = csv.reader(fin, delimiter=',', quotechar='\"')\n",
    "    header = next(reader)\n",
    "    for i, line in enumerate(reader):\n",
    "        Reviews += [line[0]]\n",
    "        Sentiments +=[int(line[1])]\n",
    "\n",
    "N=len(Reviews)\n",
    "M=len(Sentiments)\n",
    "print('Total reviews loaded', N)\n",
    "print('Total sentiments loaded', M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfed6a70",
   "metadata": {},
   "source": [
    "**Extract keywords that help to differentiate between reviews labeled as sentiment 0 and reviews labeled as sentiment 1 - Tf-Idf features to classify reviews using an SVM classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62ef88e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combination of stop words and punctuations, also get rid of br\n",
    "stop_words = stopwords.words('english') + list(punctuation)\n",
    "stop_words_set = set(stop_words) | set(['br']) | set(['p']) | set(['the']) | set(['this']) | set(['etc'])\n",
    "\n",
    "#develop tokenizer\n",
    "def tokenize(text):\n",
    "    terms = word_tokenize(text)\n",
    "    #all lower case\n",
    "    terms = [w.lower() for w in terms]\n",
    "    #filter stop words\n",
    "    terms = [w for w in terms if w not in stop_words_set and not w.isdigit()]\n",
    "    #regex for contractions and other special character strings\n",
    "    terms = [w for w in terms if not re.search(r'^\\W+|\\w\\'\\w+|\\'\\w+$', w)]\n",
    "    terms = [w for w in terms if not re.search(r'^[^a-z]+$', w)]\n",
    "    #regex for words two letters or less and numbers\n",
    "    terms = [w for w in terms if not re.search(r'^\\b\\w{1,2}\\b|(?<!\\S)\\d+(?!\\S)$', w)]\n",
    "    #lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    terms = [lemmatizer.lemmatize(w, 'n') for w in terms]\n",
    "    return terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a02dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_revs=[str(tokenize(review)) for review in Reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0114cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numerical np.array which sklearn requires\n",
    "yCategories = [0, 1]\n",
    "ydocs = np.array([yCategories.index(_) for _ in Sentiments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df0891f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StratifiedKFold will require indexable data structure\n",
    "Revs = pd.Series(all_revs)\n",
    "Sents = pd.Series(Sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a50b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_eval_docs(_clf, _Xdocs, _ydocs):\n",
    "    # Need indexable data structure\n",
    "    acc = []\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=False, random_state=None)\n",
    "    for train_index, test_index in kf.split(_Xdocs, _ydocs):\n",
    "        _clf.fit(_Xdocs[train_index], _ydocs[train_index])\n",
    "        y_pred = _clf.predict(_Xdocs[test_index])\n",
    "        acc += [accuracy_score(_ydocs[test_index], y_pred)]\n",
    "\n",
    "    return np.array(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9056f279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N data points= 50000, M features= 101895\n"
     ]
    }
   ],
   "source": [
    "#check counts, size of the X dataset - raw features\n",
    "X_tfidf = TfidfVectorizer().fit_transform(Reviews)\n",
    "print(f'N data points= {X_tfidf.shape[0]}, M features= {X_tfidf.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23b394ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 75000 features instead of 101895\n",
    "N_FEATURES= 75000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc2dddc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine (linear SVC) CV accuracy=0.897 0.003\n",
      "CPU times: user 38.6 s, sys: 695 ms, total: 39.3 s\n",
      "Wall time: 39.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#svm classifier and accuracy - accuracy tops out at 89.7% when using 75k features\n",
    "svm_lin = Pipeline([('vect', CountVectorizer(max_features=N_FEATURES)), ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', LinearSVC(class_weight='balanced'))\n",
    "                   ])\n",
    "acc = kfold_eval_docs(svm_lin, Revs, Sents)\n",
    "print(f'Support Vector Machine (linear SVC) CV accuracy={np.mean(acc):.3f} {np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4745ae1",
   "metadata": {},
   "source": [
    "**Rank the first 10 keywords that indicate the difference between the classes 0 and 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1174a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab classifier\n",
    "classifier = svm_lin.named_steps['clf']\n",
    "#grab coefficients\n",
    "coef = classifier.coef_[0].ravel()\n",
    "#get the top coefs for each sentiment\n",
    "pos_coef = np.argsort(coef)[-10:]\n",
    "neg_coef = np.argsort(coef)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49da677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the feature names\n",
    "cv = svm_lin.named_steps['vect']\n",
    "feature_names = cv.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7a23ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get sentiment 1 features and coefficients\n",
    "feat1 = feature_names[pos_coef]\n",
    "coef1 = coef[pos_coef]\n",
    "\n",
    "#get sentiment 0 features and coefficients\n",
    "feat0 = feature_names[neg_coef]\n",
    "coef0 = coef[neg_coef]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8984faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to dicts\n",
    "feat1_dict = {feat1[i]: coef1[i] for i in range(len(feat1))}\n",
    "feat0_dict = {feat0[i]: coef0[i] for i in range(len(feat0))}\n",
    "#sort dicts by value\n",
    "sort_feat1_dict = dict(sorted(feat1_dict.items(), key=lambda kv:kv[1],reverse=True))\n",
    "sort_feat0_dict = dict(sorted(feat0_dict.items(), key=lambda kv:kv[1],reverse=False))\n",
    "#convert dicts to lists\n",
    "sort_feat1_list = list(sort_feat1_dict.items())\n",
    "sort_feat0_list = list(sort_feat0_dict.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45021ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSentiment 1\t\t\tSentiment 0\n",
      "Importance\tTerm\t\tImportance\tTerm\n",
      "3.877950\texcellent       -5.810407\tworst\n",
      "3.152302\tgreat           -4.741094\twaste\n",
      "3.073947\tperfect         -4.292880\tawful\n",
      "2.894267\trefreshing      -3.662103\tboring\n",
      "2.725053\tamazing         -3.512358\tdisappointment\n",
      "2.710273\thilarious       -3.411473\tfails\n",
      "2.642047\tenjoyable       -3.200393\tpoor\n",
      "2.577578\tfavorite        -3.147187\thorrible\n",
      "2.518989\twonderful       -3.099757\tdisappointing\n",
      "2.504800\tperfectly       -3.042528\tbad\n"
     ]
    }
   ],
   "source": [
    "#print top 10 for each sentiment\n",
    "print(f\"\\tSentiment 1\\t\\t\\tSentiment 0\")\n",
    "print(f\"Importance\\tTerm\\t\\tImportance\\tTerm\")\n",
    "for i in range(10):\n",
    "    print(f\"{sort_feat1_list[i][1]:3f}\\t{sort_feat1_list[i][0]:16s}{sort_feat0_list[i][1]:3f}\\t{sort_feat0_list[i][0]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cb14d2",
   "metadata": {},
   "source": [
    "**Cluster the reviews into two groups and classify and report the 10-fold CV classification performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51f54f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N data points= 15000, M features= 62625\n"
     ]
    }
   ],
   "source": [
    "# check counts, size of the X dataset - raw features\n",
    "#cutting Reviews to 15000 - cluster would not run at full size\n",
    "X_tfidf_4 = TfidfVectorizer().fit_transform(Reviews[0:15000])\n",
    "print(f'N data points= {X_tfidf_4.shape[0]}, M features= {X_tfidf_4.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d58ec24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33min 31s, sys: 10.8 s, total: 33min 42s\n",
      "Wall time: 35min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#build clusters - Agglomerative chosen because performance is better and good for document analysis - long\n",
    "Clusters = AgglomerativeClustering(n_clusters=2, linkage='ward').fit_predict(np.array(X_tfidf_4.todense()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2946446f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Clusters: \n",
      "\n",
      "Count 0: 14070\n",
      "Count 1: 930\n"
     ]
    }
   ],
   "source": [
    "#size of clusters\n",
    "count0 = 0\n",
    "count1 = 0\n",
    "for i in Clusters:\n",
    "    if i == 1:\n",
    "        count1 += 1\n",
    "    else:\n",
    "        count0 += 1\n",
    "print(f\"Size of Clusters: \\n\\nCount 0: {count0}\\nCount 1: {count1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6505e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create svm pipeline\n",
    "svm_lin4 = Pipeline([('vect', CountVectorizer(max_features=62625)), ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', LinearSVC(class_weight='balanced'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "772b55b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine (linear SVC) CV accuracy=0.905 0.008\n",
      "CPU times: user 14 s, sys: 177 ms, total: 14.2 s\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#kfold to get the accuracy\n",
    "acc = kfold_eval_docs(svm_lin4, Revs[0:15000], Clusters)\n",
    "print(f'Support Vector Machine (linear SVC) CV accuracy={np.mean(acc):.3f} {np.std(acc):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf25366",
   "metadata": {},
   "source": [
    "**Compare keywords generated by Tf-Idf features and keywords generated by clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9648a104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tCluster - Clustering\n",
      "\tSentiment 1\t\t\tSentiment 0\n",
      "Importance\tTerm\t\tImportance\tTerm\n",
      "2.652363\tspoiler         -1.410793\tmichael \n",
      "2.425980\tmovies          -1.378715\tworking \n",
      "2.297623\tfreddy          -1.352884\tasian \n",
      "2.244643\tbridge          -1.341013\tability \n",
      "2.019728\taeon            -1.338794\tdecision \n",
      "2.009459\ttimes           -1.326530\tnegative \n",
      "1.980769\tthem            -1.293704\tclichés \n",
      "1.895242\tthis            -1.287105\tearly \n",
      "1.892824\tbecame          -1.275902\tupset \n",
      "1.881998\tgraduate        -1.254799\tempty \n",
      "\n",
      "\t\t\tClassification - TFIDF\n",
      "\tSentiment 1\t\t\tSentiment 0\n",
      "Importance\tTerm\t\tImportance\tTerm\n",
      "3.877950\texcellent       -5.810407\tworst\n",
      "3.152302\tgreat           -4.741094\twaste\n",
      "3.073947\tperfect         -4.292880\tawful\n",
      "2.894267\trefreshing      -3.662103\tboring\n",
      "2.725053\tamazing         -3.512358\tdisappointment\n",
      "2.710273\thilarious       -3.411473\tfails\n",
      "2.642047\tenjoyable       -3.200393\tpoor\n",
      "2.577578\tfavorite        -3.147187\thorrible\n",
      "2.518989\twonderful       -3.099757\tdisappointing\n",
      "2.504800\tperfectly       -3.042528\tbad\n"
     ]
    }
   ],
   "source": [
    "#grab classifier\n",
    "classifier5 = svm_lin4.named_steps['clf']\n",
    "#grab coefficients\n",
    "coef5 = classifier5.coef_[0].ravel()\n",
    "#get the top coefs for each sentiment\n",
    "pos_coef5 = np.argsort(coef5)[-10:]\n",
    "neg_coef5 = np.argsort(coef5)[:10]\n",
    "#get the feature names\n",
    "cv5 = svm_lin4.named_steps['vect']\n",
    "feature_names5 = cv5.get_feature_names_out()\n",
    "#get sentiment 1 features and coefficients\n",
    "feat15 = feature_names5[pos_coef5]\n",
    "coef15 = coef5[pos_coef5]\n",
    "#get sentiment 0 features and coefficients\n",
    "feat05 = feature_names5[neg_coef5]\n",
    "coef05 = coef5[neg_coef5]\n",
    "#convert to dicts\n",
    "feat1_dict5 = {feat15[i]: coef15[i] for i in range(len(feat15))}\n",
    "feat0_dict5 = {feat05[i]: coef05[i] for i in range(len(feat05))}\n",
    "#sort dicts by value\n",
    "sort_feat1_dict5 = dict(sorted(feat1_dict5.items(), key=lambda kv:kv[1],reverse=True))\n",
    "sort_feat0_dict5 = dict(sorted(feat0_dict5.items(), key=lambda kv:kv[1],reverse=False))\n",
    "#convert dicts to lists\n",
    "sort_feat1_list5 = list(sort_feat1_dict5.items())\n",
    "sort_feat0_list5 = list(sort_feat0_dict5.items())\n",
    "\n",
    "#print top 10 for each sentiment\n",
    "print(f\"\\t\\t\\tCluster - Clustering\")\n",
    "print(f\"\\tSentiment 1\\t\\t\\tSentiment 0\")\n",
    "print(f\"Importance\\tTerm\\t\\tImportance\\tTerm\")\n",
    "for i in range(10):\n",
    "    print(f\"{sort_feat1_list5[i][1]:3f}\\t{sort_feat1_list5[i][0]:16s}{sort_feat0_list5[i][1]:3f}\\t{sort_feat0_list5[i][0]} \")\n",
    "\n",
    "#print top 10 for each sentiment\n",
    "print(f\"\\n\\t\\t\\tClassification - TFIDF\")\n",
    "print(f\"\\tSentiment 1\\t\\t\\tSentiment 0\")\n",
    "print(f\"Importance\\tTerm\\t\\tImportance\\tTerm\")\n",
    "for i in range(10):\n",
    "    print(f\"{sort_feat1_list[i][1]:3f}\\t{sort_feat1_list[i][0]:16s}{sort_feat0_list[i][1]:3f}\\t{sort_feat0_list[i][0]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59e41a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
